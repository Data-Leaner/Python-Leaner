**决策树算法的核心是要解决两个问题**
1. 如何从数据表中找出最佳节点和最佳分枝
2. 如何让决策树停止生长，防止过拟合

## 1.2 sklearn中的决策树
|tree.DecisionTreeClassfier|分类树|
|tree.DecisionTreeRegressor|回归树|
|tree.export_graphviz   |将生成的决策树到处为DOT格式，画图专用   |
|tree.ExtraTreeClassfier   |高随机版本的分类树   |
|tree.ExtraTreeRegressor   |高随机版本的回归树   |

```python
from skearn import tree
# 实例化
clf = tree.DecisionTreeClassfier()
# 用训练集数据训练模型
clf = clf.fit(X_train,y_train)
# 导入测试集， 从接口中调用需要的信息
result = clf.score(X_test, y_test)
```

### DecisionTreeClassfier
- criterion
> 衡量最佳节点和分枝的标准是不纯度，通常来说不纯度越低，那么优化效果越好。每个节点都有不纯度，那么子节点的不纯度一定是低于父节点的。那么也就是意味着叶子节点不纯度最低。该参数主要是指不纯度的计算方法
1. entropy，使用信息熵
2. gini，使用基尼系数
$$

比起基尼系数，信息熵对不纯度更加敏感，对不纯度的惩罚最强。但是在实际使用中，信息熵和基尼系数的效果基本相同。信息熵的计算比基尼系数缓慢一些，因为基尼系数的计算不涉及对数。另外，因为信息熵对不纯度更加敏感，所以信息熵作为指标时，决策树的生产会更加精细，因此对于高维数据或者噪音很多的数据，信息熵很容易过拟合，基尼系数在这种情况下效果比较好。

不填参数的情况下：默认是基尼系数

> 通常情况下使用基尼系数，数据维度很大，噪音很大时使用基尼系数，维度低，数据比较清晰的时候，信息熵和基尼系数没区别，当决策树的拟合程度不够好的时候，使用信息熵。

```python
from sklearn import tree
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
import graphviz
import pandas as pd
wine = load_wine()
pd.concat([pd.DataFrame(wine.data), pd.DataFrame(wine.target)], axis=1)
# 三七分,30%测试集，70%训练集
Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data, wine.target, test_size=0.3)
clf = tree.DecisionTreeClassfier(criterion="entropy")
clf = clf.fit(Xtrain, Ytrain)
score = clf.score()
dot_data = tree.export_graphviz(clf,
# 特征名字
feature_names=feature_name,
# 标签名字
class_name = [],
#
filled = True
#
rounded = True
)
graph = graphviz.Source(dot_data)
print(graph)
```

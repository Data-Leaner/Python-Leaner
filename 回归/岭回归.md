## 岭回归 Ridge Regression
如果数据的特征比样本点还多，数据特征$n$，样本个数$m$, 如果$n \gt m$，则计算$(X^{T}X)^{-1}$时会出错。因为$(X^{T}X)$不是满秩矩阵，所以不可逆

岭回归最早是用来处理特征数对于样本的情况，现在也用于在估计中加入偏差，从而得到更好的估计。同样也可以解决多重共线性问题。岭回归是一种有偏估计。

- 岭回归代价函数
$J(\theta_{0}) = \frac{1}{2m}\begin{bmatrix}\sum\limits_{i=1}^{m}(h_{\theta}(x_{i}) - y_{i})^{2} + \lambda\sum\limits_{j=1}^{n}\theta_{j}^{2}\end{bmatrix}$

- 线性回归标准方程法
$$w = (X^{T}X)^{-1}X^{T}y$$

- 岭回归求解
$$w = (X^{T}X + \lambda I)^{-1}X^{T}y$$
  - $\lambda$ 为岭系数
  - $I$ 是单位矩阵

选择$\lambda$
1. 各回归系数的岭估计基本稳定
2. 残差平方和增大不太多

## 标准方程法 Normal Equation
$J(\theta) = a\theta^{2} + b\theta + c$

$J(\theta_{0}, \theta_{1},\theta_{2},...,\theta_{n})=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_{\theta}(x_i)-y_{i})^{2}$

$ X = \begin{bmatrix}
1 & 2104 & 5 & 1 & 45 \\
1 & 1416 & 3 & 2 & 40 \\
1 & 1534 & 3 & 2 & 30 \\
1 & 852 & 2 & 1 & 36
\end{bmatrix}_{4 \times 5}$

$w = \begin{bmatrix}w_{0} \\ w_{1} \\ w_{2} \\ w_{3} \\ w_{4}\end{bmatrix}_{5 \times 1}$


$y = \begin{bmatrix}460 \\ 232 \\ 315 \\ 178\end{bmatrix}_{5 \times 1}$

$\sum\limits_{i=1}^{m}(h_{\theta}(x_i)-y_{i})^{2} = (y - Xw)^{T}(y - Xw)$

:one: 分子布局(Numerator-layout): 分子为列向量或者分母为行向量
:two: 分母布局(Denominator-layout): 分子为行向量或者分母为列向量

$\frac{\partial(y-Xw)^{T}(y-Xw)}{\partial w}$

$\Rightarrow \frac{\partial (y^{T}y - y^{T}Xw - w^{T}X^{T}y + w^{T}X^{T}Xw)}{\partial w}$

$\Rightarrow \frac{\partial y^{T}y}{\partial w} - \frac{\partial  y^{T}Xw}{\partial w} - \frac{\partial w^{T}X^{T}y}{\partial w} + \frac{\partial w^{T}X^{T}Xw}{\partial w} $

$\Rightarrow \frac{\partial y^{T}y}{\partial w} = 0$

$\Rightarrow \frac{\partial  y^{T}Xw}{\partial w} = X^{T}y$

$\Rightarrow \frac{\partial w^{T}X^{T}y}{\partial w} = X^{T}y$

$\Rightarrow \frac{\partial w^{T}X^{T}Xw}{\partial w} = 2X^{T}Xw$

$\Rightarrow \frac{\partial y^{T}y}{\partial w} - \frac{\partial  y^{T}Xw}{\partial w} - \frac{\partial w^{T}X^{T}y}{\partial w} + \frac{\partial w^{T}X^{T}Xw}{\partial w} = 0 - X^{T}y - X^{T}y + 2X^{T}Xw$

$\Rightarrow -2X^{T}y + 2X^{T}Xw = 0$

$\Rightarrow X^{T}Xw = X^{T}y$

$\Rightarrow (X^{T}X)^{-1}X^{T}Xw = (X^{T}X)^{-1}X^{T}y$

$\Rightarrow w = (X^{T}X)^{-1}X^{T}y$


:ng_man: 线性相关的特征,多重共线性，矩阵不可逆
:ng_man: 特征数据太多，样本数量小于特征数量，矩阵不可逆

求解法|优点|缺点
-----|-----|-----
梯度下降法  | 当特征值特别多的时候也可以很好的工作   | 需要选择合适的学习率，需要迭代很多个周期，只能得到最优解的近似解
标准方程法  | 不需要学利率，不需要迭代，可以得到全局最优解   | 需要计算$(X^{T}X)^{-1}$，时间复杂度为$O(n^{3})$，n是特征向量

```python
import numpy as np
from numpy import genfromtxt
import matplotlib.pyplot as plt

data = np.genfromtxt("data.csv", delimiter=",")
x_data = data[:,0,np.newaxis]
y_data = data[:,1,np.newaxis]
plt.scatter(x_data, y_data)
plt.show()

print(np.mat(x_data).shape)
print(np.mat(y_data).shape)
# 给样本添加偏置项
X_data = np.concatenate((np.ones(100,1), x_data), axis=1)
# 标准方程法求解回归参数
def weights(xArr, yArr):
  xMat = np.mat(xArr)
  yMat = np.mat(yArr)
  xTx = xMat.T * xMat
  # 计算矩阵的值，如果为0，说明该矩阵没有逆矩阵
  if np.linalg.det(xTx) == 0.0:
    print("This matrix is not inverse")
    return
  else:
    return = xTx.I * xMat.T * yMat

ws = weights(X_data, y_data)

```

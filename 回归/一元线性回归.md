## 一元线性回归
- 回归分析(regression analysis)用来建立方程模拟两个或者多个变量之间如何关联
- 被预测的变量叫做：因变量(dependent variable)，输出(output)
- 被用来进行预测的变量叫做：自变量(independent variable)，输入(input)
- 一元线性回归包含一个自变量和一个因变量
- 以上两个变量的关系用一条直线来模拟
- 如果包含两个以上的自变量，则称作多元回归分析(multiple regression)

$h_{\theta}(x) = \theta_{1}(x) + \theta_{0}$
这个方程对应的图像是一条直线，称作回归线，其中$\theta_{1}$为回归线的斜率，$\theta_{0}$称为回归线的截距

## 代价函数 cost function, 损失函数 lost function
- 最小二乘法
- 真实值$y$，预测值$h_{\theta}{x}$，则误差平方为$(y-h_{\theta}(x))^2$
- 找到合适的参数，使得误差平方和$J(\theta_{0},\theta{1})=\frac{1}{2m}\sum\limits_{i=1}^{m}(y^{i}-h_{\theta}(x^{i}))^2$最小

损失函数  | Hypothesis  | Simplified
--|---|--
function  | $h_{\theta}(x) = \theta_{0} + \theta_{1}(x)$   |  $h_{\theta}(x) = \theta_{1}(x)$
parameter  | $\theta_{0}$, $\theta_{1}$   | $\theta_{1}$
cost function  | $J(\theta_{0},\theta{1})=\frac{1}{2m}\sum\limits_{i=1}^{m}(y^{i}-h_{\theta}(x^{i}))^2$  | $J(\theta{1})=\frac{1}{2m}\sum\limits_{i=1}^{m}(y^{i}-h_{\theta}(x^{i}))^2$
Goal  |$minimizeJ(\theta_{0}, \theta_{1})$   |  $minimizeJ(\theta_{1})$
**损失函数是可以多种多样的，上述的参数$\frac{1}{2}$只是为了方便计算没有特殊含义**

## 相关系数
:star: 使用相关系数去衡量线性关系相关性的强弱
**$r_{XY} = \frac{\sum(x_{i} - \bar x)(y_{i} - \bar y)}{\sqrt{\sum(x_{i} - \bar x)^2\sum(y_{i} - \bar y)^2}}$**

:smile: 相关系数越大越好

## 决定系数
相关系数$R^2$(coeffcient of determination)是用来描述连那个变量之间的线性关系的，但决定系数的适用范围更广，可以用于描述非线性或者有两个及两个以上自变量的相关关系。它可以用来评价模型效果

:one: 总平方和(SST): $\sum\limits_{i=1}^{n}(y_{i} - \bar y)^{2}$
:two: 回归平方和(SSR)：$\sum\limits_{i=1}^{n}(\widehat y_{i} - \bar y)^{2}$
:three: 残差平方和(SSE): $\sum\limits_{i=1}^{n}(y_{i} - \widehat y)^{2}$
:star: 三者之间的关系：SST = SSR + SSE

**决定系数：** $R^{2} = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$

- $y_{i}$： 真实值
- $\widehat y$: 预测值
- $\bar y$: 平均值

:smile: 结果越靠近1，越接近线性的结果

## 梯度下降法(gradient descent)
have some function: $J(\theta_{0}, \theta_{1})$
want: $\min\limits_{\theta_{0}, \theta_{1}}(\theta_{0}, \theta_{1})$
- 初始化: $\theta_{0}$, $\theta_{1}$
- 不断改变 $\theta_{0}$, $\theta_{1}$，直到$J(\theta_{0}, \theta_{1})$达到一个全局最小值，或局部最小值

**起始值的选择可以影响结果**


`repeat until convergence` {
  $\theta_{j} := \theta_{j} - \alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0}, \theta_{1})$`(for j = 0 and j = 1)`
}
:star: 学习率

正确做法： 同步更新
$temp0 := \theta_{0} - \alpha\frac{\partial}{\partial\theta_{0}}J(\theta_{0}, \theta_{1})$
$temp1 := \theta_{1} - \alpha\frac{\partial}{\partial\theta_{1}}J(\theta_{0}, \theta_{1})$
$\theta_{0} := temp0$
$\theta_{1} := temp1$

不正确的做法
$temp0 := \theta_{0} - \alpha\frac{\partial}{\partial\theta_{0}}J(\theta_{0}, \theta_{1})$
$\theta_{0} := temp0$
$temp1 := \theta_{1} - \alpha\frac{\partial}{\partial\theta_{1}}J(\theta_{0}, \theta_{1})$
$\theta_{1} := temp1$

**注意计算顺序**
### 梯度下降法求解线性回归
:one: $temp0 = \frac{\partial}{\partial\theta_{j}}J(\theta_{0}, \theta_{1})$
:two: $J(\theta_{0}, \theta_{1})=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{i}) - y^{i})^2$
:three: $h_{\theta}(x) = \theta_{1}x + \theta_{0}$
$\Rightarrow J(\theta_{0}, \theta_{1})=\frac{1}{2m}\sum\limits_{i=1}^{m}(\theta_{1}x^{i} + \theta_{0} - y^{i})^2$
$\Rightarrow \frac{\partial}{\partial\theta_{0}}J(\theta_{0}, \theta_{1}) = \frac{1}{m}\sum\limits_{i=1}^{m}(\theta_{1}x^{i} + \theta_{0} - y^{i})(1 + 0 - 0)$
$\Rightarrow \frac{\partial}{\partial\theta_{0}}J(\theta_{0}, \theta_{1}) = \frac{1}{m}\sum\limits_{i=1}^{m}(\theta_{1}x^{i} + \theta_{0} - y^{i})$
$\Rightarrow \frac{\partial}{\partial\theta_{1}}J(\theta_{0}, \theta_{1}) = \frac{1}{m}\sum\limits_{i=1}^{m}(\theta_{1}x^{i} + \theta_{0} - y^{i})(x^{i} + 0 - 0)$
$\Rightarrow \frac{\partial}{\partial\theta_{1}}J(\theta_{0}, \theta_{1}) = \frac{x^{i}}{m}\sum\limits_{i=1}^{m}(\theta_{1}x^{i} + \theta_{0} - y^{i})$
$temp0 = \theta_{0} - \alpha\frac{1}{m}\sum\limits_{i=1}^{m}(\theta_{1}x^{i} + \theta_{0} - y^{i})$
$temp1 = \theta_{1} - \alpha\frac{x^{i}}{m}\sum\limits_{i=1}^{m}(\theta_{1}x^{i} + \theta_{0} - y^{i})$

**线性回归的代价是凸函数，非凸函数可能得到很多局部极小值**

```python
import numpy as np
import matplotlib.pyplot as plt

# 加载数据
data = np.genfromtxt("data.csv", delimiter=",")
x_data = data[:,0]
y_data = data[;,1]
plt.scatter(x_data, y_data)
plt.show()

# learning rate
lr = 0.0001
# 截距
b = 0
# 斜率
k = 0
# 最大迭代次数
epochs = 50

# 最小二乘法
def compute_error(b, k, x_data, y_data):
  totalError = 0
  for i in range(0, len(x_data)):
    totalError += (y_data[i] - (k * x_data[i] + b)) ** 2
  return totalError/float(len(x_data))

def gradient_descent_runner(x_data, y_data, b, k, lr, epochs):
  # 计算总数据量
  m = float(len(x_data))
  for i in range(len(epochs)):
    b_grad = 0
    k_grad = 0
    for j in range(0, len(x_data)):
      b_grad += (1/m)*(((k* x_data[j]) + b)-y_data[j])
      k_grad += (1/m)*x_data[j]*(((k* x_data[j]) + b)-y_data[j])
    b = b - (lr * b_grad)
    k = k - (lr * k_grad)
  return b,k
print("Starting b={0}, k={1}, error={2}".format(b,k,compute_error(b,k,x_data,y_data)))
print("running")
b,k = gradient_descent_runner(x_data,y_data,b,k,lr,epochs)
print("After {0} iterations={1},b={2},error={3}".format(epochs, b, k, compute_error(b,k,x_data,y_data)))
plt.plot(x_data,y_data,'b')
plt.plot(x_data,k*x_data+b,'r')
plt.show()
```

- sklearn
```python
from sklearn.linear_model import LinearRegression
from numpy as np
from matplotlib.pyplot as plt
data = np.genfromtxt('data.csv', delimiter=",")
x_data = data[:,0,np.newaxis]
y_data = data[:,1,np.newaxis]
model = LinearRegression()
model.fit(x_data, y_data)
model.predict(x_data)
```
